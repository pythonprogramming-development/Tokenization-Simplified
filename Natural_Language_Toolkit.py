import nltk
from nltk.tokenize import TreebankWordTokenizer, WordPunctTokenizer, RegexpTokenizer, TweetTokenizer, PunktSentenceTokenizer, WhitespaceTokenizer

# Sample text for tokenization
text = "He said, 'Hello!' #greeting @someone ðŸ˜Š How are you?"

def displayTokenizationResults(tokenizer_name, tokens):
    """
        Displays the tokenization results in a clear and formatted manner.
          Args:
              tokenizer_name: The name of the tokenizer used for tokenization.
              tokens: A list of tokens generated by the tokenizer.
    """
    # Type and value checks
    if not isinstance(tokenizer_name, str):
        raise TypeError("`tokenizer_name` must be a string.")
    if not isinstance(tokens, list):
        raise TypeError("`tokens` must be a list.")
    
    # Ensure inputs are not empty
    if not tokenizer_name:
        raise ValueError("`tokenizer_name` cannot be empty.")
    if not tokens:
        raise ValueError("`tokens` cannot be empty.")
    
    # Display the tokenizer name and results
    print(f"\n{tokenizer_name}:\n{'-'*len(tokenizer_name)}")
    for token in tokens:
        print(token)
  
  
# 1. TreebankWordTokenizer - Tokenizes text based on the Penn Treebank conventions.
treebank_tokenizer = TreebankWordTokenizer()
treebank_tokens = treebank_tokenizer.tokenize(text)
displayTokenizationResults("TreebankWordTokenizer", treebank_tokens)

# 2. WordPunctTokenizer - Splits text into words and punctuation.
wordpunct_tokenizer = WordPunctTokenizer()
wordpunct_tokens = wordpunct_tokenizer.tokenize(text)
displayTokenizationResults("WordPunctTokenizer", wordpunct_tokens)

# 3. RegexpTokenizer - Tokenizes using regular expressions.
regexp_tokenizer = RegexpTokenizer(r'\w+')
regexp_tokens = regexp_tokenizer.tokenize(text)
displayTokenizationResults("RegexpTokenizer (only words)", regexp_tokens)

# 4. TweetTokenizer - Handles tokenization of tweets (hashtags, mentions, emojis).
tweet_tokenizer = TweetTokenizer()
tweet_tokens = tweet_tokenizer.tokenize(text)
displayTokenizationResults("TweetTokenizer", tweet_tokens)

# 5. SentTokenizer (Punkt) - Splits text into sentences.
sent_tokenizer = PunktSentenceTokenizer()
sentences = sent_tokenizer.tokenize(text)
displayTokenizationResults("SentTokenizer (Punkt)", sentences)

# 6. WhitespaceTokenizer - Splits text based on whitespaces.
whitespace_tokenizer = WhitespaceTokenizer()
whitespace_tokens = whitespace_tokenizer.tokenize(text)
displayTokenizationResults("WhitespaceTokenizer", whitespace_tokens)
